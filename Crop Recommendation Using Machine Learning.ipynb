code for crop recommendation

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Replace 'Crop_recommendation (1).csv' with the correct path to your file
df = pd.read_csv('Crop_recommendation Updated.csv')

# View the first few rows to understand the data structure
print(df.head())

# Fill missing values only for numeric columns
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())

# Alternatively, fill non-numeric columns separately if needed
# df['label'] = df['label'].fillna('Unknown')  # Example for categorical columns

# Separate numeric and categorical columns
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
categorical_columns = df.select_dtypes(include=['object']).columns

# Get the number of rows and columns
num_rows, num_columns = df.shape
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_columns}")

# Get column names
columns = df.columns
print("Column names:")
print(columns)

# Get data types of each column
data_types = df.dtypes
print("Data types of columns:")
print(data_types)

df.label.unique()

df['label'].unique().size

# Select only numeric columns for mean imputation
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Fill missing values for numeric columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

print(df.columns)
df.columns = df.columns.str.strip()

print(df.dtypes)
# Import necessary libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate correlation matrix
numeric_df = df.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numeric_df.corr()

# Plot the heatmap
sns.heatmap(correlation_matrix, annot=True)
plt.show()


# Import necessary libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier



# Assume 'label' is the target variable, and the rest are features
X = df.drop('label', axis=1)  # Features (independent variables)
y = df['label']  # Target (dependent variable)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Initialize the RandomForestClassifier model
model = RandomForestClassifier()

# Fit the model with the training data
model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")


from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # Import confusion_matrix

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Get a detailed classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()


# Import necessary libraries
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier

# Hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Initialize the RandomForestClassifier model

# Import necessary libraries
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier
import joblib

# Hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30] }


# Replace 'label' with the name of your target column
X = df.drop(columns=['label'])  # Features
y = df['label']  # Target

# Verify the split
print(X.shape, y.shape)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shapes of the splits
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)


# Initialize the model
model = RandomForestClassifier(random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.preprocessing import MinMaxScaler, StandardScaler
sc = StandardScaler()

sc.fit(X_train)  # Fit StandardScaler

from sklearn.preprocessing import MinMaxScaler
import pickle

# Assuming you have training data
scaler = MinMaxScaler()
scaler.fit(X_train)  # Fit it on the training data
pickle.dump(scaler, open('minmaxscaler.pkl', 'wb'))

# Example input: Replace these values with the actual values for the features
input_data = {
    'N': 150,
    'P': 128,
    'K': 33,
    'temperature': 23.5,
    'humidity': 67.2,
    'ph': 6.3,
    'rainfall': 259.0
}

# Convert input data to a DataFrame
input_df = pd.DataFrame([input_data])

# Predict the label (crop recommendation)
predicted_label = model.predict(input_df)

# Display the result
print(f"Recommended Crop: {predicted_label[0]}")


code for fertilizer suggestion

from google.colab import drive
drive.mount('/content/drive')
%ls
%cd /content/drive/MyDrive/Final Year Project/Fertillizer Suggestion


%ls

#importing libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import warnings
warnings.filterwarnings('ignore')

#importing the dataset
data = pd.read_csv('f2.csv')
data.head()
data.info()

#changing the column names
data.rename(columns={'Humidity ':'Humidity','Soil Type':'Soil_Type','Crop Type':'Crop_Type','Fertilizer Name':'Fertilizer'},inplace=True)

#checking unique values
data.nunique()

#checking for null values
data.isna().sum()


data['Fertilizer'].unique()

data['Crop_Type'].unique()


#statistical parameters
data.describe(include='all')


plt.figure(figsize=(13, 5))
sns.set(style="whitegrid")
sns.countplot(data=data, x='Crop_Type')
plt.title('Count Plot for Crop_Type')
plt.xlabel('Crop_Type')
plt.ylabel('Count')
plt.show()



#first 4 crop types
part1_data = data[data['Crop_Type'].isin(data['Crop_Type'].value_counts().index[:4])]

# Create the first countplot
plt.figure(figsize=(10, 4))
sns.set(style="whitegrid")
sns.countplot(data=part1_data, x='Crop_Type', hue='Fertilizer', width=0.8, palette='Set2')
plt.title('First 4 Crop Types')
plt.xlabel('Crop_Type')
plt.ylabel('Count')
plt.legend(title='Fertilizer')
plt.xticks(rotation=45, horizontalalignment='right')
plt.tight_layout()
plt.show()

# Split the data into three parts: next 4 crop types
part2_data = data[data['Crop_Type'].isin(data['Crop_Type'].value_counts().index[4:8])]

# Create the second countplot
plt.figure(figsize=(8, 4))
sns.set(style="whitegrid")
sns.countplot(data=part2_data, x='Crop_Type', hue='Fertilizer', width=0.8, palette='Set2')
plt.title('Next 4 Crop Types')
plt.xlabel('Crop_Type')
plt.ylabel('Count')
plt.legend(title='Fertilizer')
plt.xticks(rotation=45, horizontalalignment='right')
plt.tight_layout()
plt.show()


# Split the data into three parts: remaining crop types
part3_data = data[data['Crop_Type'].isin(data['Crop_Type'].value_counts().index[8:13])]

# Create the third countplot
plt.figure(figsize=(8, 4))
sns.set(style="whitegrid")
sns.countplot(data=part3_data, x='Crop_Type', hue='Fertilizer', width=0.8, palette='Set2')
plt.title('Remaining Crop Types')
plt.xlabel('Crop_Type')
plt.ylabel('Count')
plt.legend(title='Fertilizer')
plt.xticks(rotation=45, horizontalalignment='right')
plt.tight_layout()
plt.show()

# Select only numeric columns from the data
numeric_data = data.select_dtypes(include=['number'])

# Generate the heatmap for the correlation matrix
sns.heatmap(numeric_data.corr(), annot=True)

#encoding the labels for categorical variables
from sklearn.preprocessing import LabelEncoder
#it  transforming non-numeric data into a numeric format


#encoding Soil Type variable
encode_soil = LabelEncoder()

#fitting the label encoder
data.Soil_Type = encode_soil.fit_transform(data.Soil_Type)

#creating the DataFrame
Soil_Type = pd.DataFrame(zip(encode_soil.classes_,encode_soil.transform(encode_soil.classes_)),columns=['Original','Encoded'])
Soil_Type = Soil_Type.set_index('Original')
Soil_Type

#encoding Crop Type variable
encode_crop = LabelEncoder()

#fitting the label encoder
data.Crop_Type = encode_crop.fit_transform(data.Crop_Type)

#creating the DataFrame
Crop_Type = pd.DataFrame(zip(encode_crop.classes_,encode_crop.transform(encode_crop.classes_)),columns=['Original','Encoded'])
Crop_Type = Crop_Type.set_index('Original')
Crop_Type

#encoding Fertilizer variable
encode_ferti = LabelEncoder()

#fitting the label encoder
data.Fertilizer = encode_ferti.fit_transform(data.Fertilizer)

#creating the DataFrame
Fertilizer = pd.DataFrame(zip(encode_ferti.classes_,encode_ferti.transform(encode_ferti.classes_)),columns=['Original','Encoded'])
Fertilizer = Fertilizer.set_index('Original')
Fertilizer

#splitting the data into train and test
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(data.drop('Fertilizer',axis=1),data.Fertilizer,test_size=0.2,random_state=1)
print('Shape of Splitting :')
print('x_train = {}, y_train = {}, x_test = {}, y_test = {}'.format(x_train.shape,y_train.shape,x_test.shape,y_test.shape))


x_train.info()
acc = [] # TEST
model = []
acc1=[] # TRIAN

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
from sklearn import metrics
import warnings

warnings.filterwarnings('ignore')

# Initialize the Decision Tree classifier
ds = DecisionTreeClassifier(criterion="entropy", random_state=2, max_depth=5)
ds.fit(x_train, y_train)

# Predict on the test data
predicted_test_values = ds.predict(x_test)
test_accuracy = metrics.accuracy_score(y_test, predicted_test_values)
acc.append(test_accuracy)
model.append('Decision Tree')

# Predict on the training data
predicted_train_values = ds.predict(x_train)
train_accuracy = metrics.accuracy_score(y_train, predicted_train_values)
acc1.append(train_accuracy)

# Print accuracy for both train and test
print("Decision Tree's Test Accuracy: ", test_accuracy * 100)
print("Decision Tree's Train Accuracy: ", train_accuracy * 100)

# Print classification report for the test data
print(classification_report(y_test, predicted_test_values))

from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.metrics import classification_report

# Initialize the Naive Bayes classifier
NaiveBayes = GaussianNB()

# Fit the model on the training data
NaiveBayes.fit(x_train, y_train)

# Predict on the test data
predicted_test_values = NaiveBayes.predict(x_test)
test_accuracy = metrics.accuracy_score(y_test, predicted_test_values)
acc.append(test_accuracy)

# Predict on the training data
predicted_train_values = NaiveBayes.predict(x_train)
train_accuracy = metrics.accuracy_score(y_train, predicted_train_values)
acc1.append(train_accuracy)

# Append the model name to the list
model.append('Naive Bayes')

# Print the accuracies
print("Naive Bayes's Test Accuracy: ", test_accuracy)
print("Naive Bayes's Train Accuracy: ", train_accuracy)

# Print the classification report for the test data
print(classification_report(y_test, predicted_test_values))

from sklearn.svm import SVC
from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics
from sklearn.metrics import classification_report

# Data normalization with sklearn
norm = MinMaxScaler().fit(x_train)
X_train_norm = norm.transform(x_train)
X_test_norm = norm.transform(x_test)

# Initialize and fit the SVM classifier
SVM = SVC(kernel='poly', degree=3, C=1)
SVM.fit(X_train_norm, y_train)

# Predict on the test data
predicted_test_values = SVM.predict(X_test_norm)
test_accuracy = metrics.accuracy_score(y_test, predicted_test_values)
acc.append(test_accuracy)

# Predict on the training data
predicted_train_values = SVM.predict(X_train_norm)
train_accuracy = metrics.accuracy_score(y_train, predicted_train_values)
acc1.append(train_accuracy)

# Append the model name to the list
model.append('SVM')

# Print the accuracies
print("SVM's Test Accuracy: ", test_accuracy)
print("SVM's Train Accuracy: ", train_accuracy)

# Print the classification report for the test data
print(classification_report(y_test, predicted_test_values))

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import classification_report

# Initialize the Logistic Regression model
LogReg = LogisticRegression(random_state=2)

# Fit the model on the training data
LogReg.fit(x_train, y_train)

# Predict on the test data
predicted_test_values = LogReg.predict(x_test)
test_accuracy = metrics.accuracy_score(y_test, predicted_test_values)
acc.append(test_accuracy)

# Predict on the training data
predicted_train_values = LogReg.predict(x_train)
train_accuracy = metrics.accuracy_score(y_train, predicted_train_values)
acc1.append(train_accuracy)

# Append the model name to the list
model.append('Logistic Regression')

# Print the accuracies
print("Logistic Regression's Test Accuracy: ", test_accuracy)
print("Logistic Regression's Train Accuracy: ", train_accuracy)

# Print the classification report for the test data
print(classification_report(y_test, predicted_test_values))


from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import classification_report

# Initialize the Random Forest classifier
RF = RandomForestClassifier(n_estimators=20, random_state=0)

# Fit the model on the training data
RF.fit(x_train, y_train)

# Predict on the test data
predicted_test_values = RF.predict(x_test)
test_accuracy = metrics.accuracy_score(y_test, predicted_test_values)
acc.append(test_accuracy)

# Predict on the training data
predicted_train_values = RF.predict(x_train)
train_accuracy = metrics.accuracy_score(y_train, predicted_train_values)
acc1.append(train_accuracy)

# Append the model name to the list
model.append('RF')

# Print the accuracies
print("RF's Test Accuracy: ", test_accuracy)
print("RF's Train Accuracy: ", train_accuracy)

# Print the classification report for the test data
print(classification_report(y_test, predicted_test_values))

from sklearn.model_selection import cross_val_score

score = cross_val_score(RF,data,data.Fertilizer,cv=5)
print("Cross-validation score of RF is:",score)
score = cross_val_score(LogReg,data,data.Fertilizer,cv=5)
print("Cross-validation score of LogReg is:",score)
score = cross_val_score(SVM,data,data.Fertilizer,cv=5)
print("Cross-validation score of SVM is:",score)
score = cross_val_score(NaiveBayes,data,data.Fertilizer,cv=5)
print("Cross-validation score of NaiveBayes is:",score)
score = cross_val_score(ds, data, data.Fertilizer,cv=5)
print("Cross-validation score of ds is:",score)


plt.figure(figsize=[10,5],dpi = 100)
plt.title('Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Algorithm')
sns.barplot(x = acc,y = model,palette='dark')

import pickle
import numpy as np

# Load the trained model
model = pickle.load(open('classifier.pkl', 'rb'))

# Load the encoder for fertilizers
ferti = pickle.load(open('fertilizer.pkl', 'rb'))

# Input values (make sure this is in the same format as training data)
input_data = [[100,24,61,2,2,32,2,10]]

# Predict using the model
prediction = model.predict(input_data)

# Decode the prediction using the fertilizer encoder
predicted_fertilizer = ferti.inverse_transform([prediction[0]])

print(f"Predicted Fertilizer Class: {predicted_fertilizer[0]}")


crop yield prediction

%cd /content/drive/MyDrive/Final Year Project/Crop Yield Prediction


%cd /content/drive/MyDrive/Final Year Project/Crop Yield Prediction

%ls

#importing necessary libraries
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# loading the dataset

crop_data=pd.read_csv("crop_production.csv")
crop_data['Crop'] = crop_data['Crop'].str.lower()
#lst = list(crop_data['Crop'].str.lower().unique())
#lst.sort()
#print(lst)

crop_data['Crop'] = crop_data['Crop'].replace(['moth','peas  (vegetable)','bean','moong(green gram)','pome granet','water melon','cotton(lint)','gram'],['mothbeans','pigeonpeas','kidneybeans','mungbean','pomegranate','watermelon','cotton','chickpea'])

crop_data = crop_data[crop_data['Crop'].isin(['rice', 'maize', 'chickpea', 'kidneybeans', 'pigeonpeas','mothbeans', 'mungbean', 'blackgram', 'lentil', 'pomegranate','banana', 'mango', 'grapes', 'watermelon', 'apple','orange', 'papaya', 'coconut', 'cotton', 'jute', 'coffee'])]

crop_data = crop_data.drop(['State_Name','District_Name'],axis = 1)

crop_data
crop_data['Crop'].unique()

# dataset columns
crop_data.columns


crop_data.describe()

# Checking missing values of the dataset in each column
crop_data.isnull().sum()

# Dropping missing values
crop_data = crop_data.dropna()
crop_data

#checking
crop_data.isnull().values.any()


# Visualizing the features

ax = sns.pairplot(crop_data)
ax
data = crop_data

# Convert non-numeric columns to numeric, if applicable, or drop them
data_numeric = data.select_dtypes(include=[float, int])

# Now compute the correlation matrix
correlation_matrix = data_numeric.corr()
print(correlation_matrix)
import seaborn as sns
import matplotlib.pyplot as plt

# Select only numeric columns from the dataset
data_numeric = data.select_dtypes(include=[float, int])

# Calculate the correlation matrix
correlation_matrix = data_numeric.corr()

# Plot the heatmap
sns.heatmap(correlation_matrix, annot=True)
plt.title('Correlation Matrix')
plt.show()
dummy = pd.get_dummies(data)
dummy
from sklearn.model_selection import train_test_split

x = dummy.drop(["Production"], axis=1)
y = dummy["Production"]

# Splitting data set - 25% test dataset and 75% train

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25, random_state=5)

print("x_train :",x_train.shape)
print("x_test :",x_test.shape)
print("y_train :",y_train.shape)
print("y_test :",y_test.shape)


from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 11)
model.fit(x_train,y_train)
rf_predict = model.predict(x_test)
rf_predict

model.score(x_test,y_test)


from sklearn.metrics import r2_score
r1 = r2_score(y_test,rf_predict)
print("R2 score : ",r1)

# Calculating Adj. R2 score:

Adjr2_1 = 1 - (1-r1)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)
print("Adj. R-Squared : {}".format(Adjr2_1))


ax = sns.distplot(y_test, hist = False, color = "r", label = "Actual value ")
sns.distplot(rf_predict, hist = False, color = "b", label = "Predicted Values", ax = ax)
plt.title('Random Forest Regression')

inp = [2001, 'Kharif', 'maize', 83]

test_row = x_test.head(1)
test_row['Crop_Year'] = inp[0]

string = ""
for i in test_row.columns[2:]:
    string = str(i)
    if inp[1] in string or inp[2] in string:
        test_row[i] = 1
    else:
        test_row[i] = 0

test_row['Area'] = inp[3]

production = model.predict(test_row)[0]

yd = production / test_row['Area']
print("production: ", production)
for ind, val in yd.items():  # Use .items() instead of .iteritems()
    print("yield: ", val)



plant disease detection
-------------------------

!pip install torchsummary

import os                       # for working with files
import numpy as np              # for numerical computationss
import pandas as pd             # for working with dataframes
import torch                    # Pytorch module 
import matplotlib.pyplot as plt # for plotting informations on graph and images using tensors
import torch.nn as nn           # for creating  neural networks
from torch.utils.data import DataLoader # for dataloaders 
from PIL import Image           # for checking images
import torch.nn.functional as F # for functions for calculating loss
import torchvision.transforms as transforms   # for transforming images into tensors 
from torchvision.utils import make_grid       # for data checking
from torchvision.datasets import ImageFolder  # for working with classes and images
from torchsummary import summary              # for getting the summary of our model

%matplotlib inline

data_dir = "../input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)"
train_dir = data_dir + "/train"
valid_dir = data_dir + "/valid"
diseases = os.listdir(train_dir)



# printing the disease names
print(diseases)

print("Total disease classes are: {}".format(len(diseases)))

plants = []
NumberOfDiseases = 0
for plant in diseases:
    if plant.split('___')[0] not in plants:
        plants.append(plant.split('___')[0])
    if plant.split('___')[1] != 'healthy':
        NumberOfDiseases += 1

# unique plants in the dataset
print(f"Unique Plants are: \n{plants}")

# number of unique plants
print("Number of plants: {}".format(len(plants)))

# number of unique diseases
print("Number of diseases: {}".format(NumberOfDiseases))

# Number of images for each disease
nums = {}
for disease in diseases:
    nums[disease] = len(os.listdir(train_dir + '/' + disease))
    
# converting the nums dictionary to pandas dataframe passing index as plant name and number of images as column

img_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=["no. of images"])
img_per_class

# plotting number of images available for each disease
index = [n for n in range(38)]
plt.figure(figsize=(20, 5))
plt.bar(index, [n for n in nums.values()], width=0.3)
plt.xlabel('Plants/Diseases', fontsize=10)
plt.ylabel('No of images available', fontsize=10)
plt.xticks(index, diseases, fontsize=5, rotation=90)
plt.title('Images per each class of plant disease')


n_train = 0
for value in nums.values():
    n_train += value
print(f"There are {n_train} images for training")


n_train = 0
for value in nums.values():
    n_train += value
print(f"There are {n_train} images for training")

img, label = train[0]
print(img.shape, label)

# total number of classes in train set
len(train.classes)


# for checking some images from training dataset
def show_image(image, label):
    print("Label :" + train.classes[label] + "(" + str(label) + ")")
    plt.imshow(image.permute(1, 2, 0))

show_image(*train[0])


show_image(*train[70000])

show_image(*train[30000])

# Setting the seed value
random_seed = 7
torch.manual_seed(random_seed)

# setting the batch size
batch_size = 32

# DataLoaders for training and validation
train_dl = DataLoader(train, batch_size, shuffle=True, num_workers=2, pin_memory=True)
valid_dl = DataLoader(valid, batch_size, num_workers=2, pin_memory=True)

# helper function to show a batch of training instances
def show_batch(data):
    for images, labels in data:
        fig, ax = plt.subplots(figsize=(30, 30))
        ax.set_xticks([]); ax.set_yticks([])
        ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))
        break

# Images for first batch of training
show_batch(train_dl) 

# for moving data into GPU (if available)
def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available:
        return torch.device("cuda")
    else:
        return torch.device("cpu")

# for moving data to device (CPU or GPU)
def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

# for loading in the device (GPU if available else CPU)
class DeviceDataLoader():
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device
        
    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl:
            yield to_device(b, self.device)
        
    def __len__(self):
        """Number of batches"""
        return len(self.dl)

device = get_default_device()
device

# Moving data into GPU
train_dl = DeviceDataLoader(train_dl, device)
valid_dl = DeviceDataLoader(valid_dl, device)

class SimpleResidualBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        
    def forward(self, x):
        out = self.conv1(x)
        out = self.relu1(out)
        out = self.conv2(out)
        return self.relu2(out) + x # ReLU can be applied before or after adding the input

# for calculating the accuracy
def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))


# base class for the model
class ImageClassificationBase(nn.Module):
    
    def training_step(self, batch):
        images, labels = batch
        out = self(images)                  # Generate predictions
        loss = F.cross_entropy(out, labels) # Calculate loss
        return loss
    
    def validation_step(self, batch):
        images, labels = batch
        out = self(images)                   # Generate prediction
        loss = F.cross_entropy(out, labels)  # Calculate loss
        acc = accuracy(out, labels)          # Calculate accuracy
        return {"val_loss": loss.detach(), "val_accuracy": acc}
    
    def validation_epoch_end(self, outputs):
        batch_losses = [x["val_loss"] for x in outputs]
        batch_accuracy = [x["val_accuracy"] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()       # Combine loss  
        epoch_accuracy = torch.stack(batch_accuracy).mean()
        return {"val_loss": epoch_loss, "val_accuracy": epoch_accuracy} # Combine accuracies
    
    def epoch_end(self, epoch, result):
        print("Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}".format(
            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_accuracy']))
        
# Architecture for training

# convolution block with BatchNormalization
def ConvBlock(in_channels, out_channels, pool=False):
    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
             nn.BatchNorm2d(out_channels),
             nn.ReLU(inplace=True)]
    if pool:
        layers.append(nn.MaxPool2d(4))
    return nn.Sequential(*layers)


# resnet architecture 
class ResNet9(ImageClassificationBase):
    def __init__(self, in_channels, num_diseases):
        super().__init__()
        
        self.conv1 = ConvBlock(in_channels, 64)
        self.conv2 = ConvBlock(64, 128, pool=True) # out_dim : 128 x 64 x 64 
        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))
        
        self.conv3 = ConvBlock(128, 256, pool=True) # out_dim : 256 x 16 x 16
        self.conv4 = ConvBlock(256, 512, pool=True) # out_dim : 512 x 4 x 44
        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))
        
        self.classifier = nn.Sequential(nn.MaxPool2d(4),
                                       nn.Flatten(),
                                       nn.Linear(512, num_diseases))
        
    def forward(self, xb): # xb is the loaded batch
        out = self.conv1(xb)
        out = self.conv2(out)
        out = self.res1(out) + out
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.res2(out) + out
        out = self.classifier(out)
        return out        

# defining the model and moving it to the GPU
model = to_device(ResNet9(3, len(train.classes)), device) 
model


# getting summary of the model
INPUT_SHAPE = (3, 256, 256)
print(summary(model.cuda(), (INPUT_SHAPE)))

# for training
@torch.no_grad()
def evaluate(model, val_loader):
    model.eval()
    outputs = [model.validation_step(batch) for batch in val_loader]
    return model.validation_epoch_end(outputs)


def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']
    

def fit_OneCycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,
                grad_clip=None, opt_func=torch.optim.SGD):
    torch.cuda.empty_cache()
    history = []
    
    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)
    # scheduler for one cycle learniing rate
    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))
    
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_losses = []
        lrs = []
        for batch in train_loader:
            loss = model.training_step(batch)
            train_losses.append(loss)
            loss.backward()
            
            # gradient clipping
            if grad_clip: 
                nn.utils.clip_grad_value_(model.parameters(), grad_clip)
                
            optimizer.step()
            optimizer.zero_grad()
            
            # recording and updating learning rates
            lrs.append(get_lr(optimizer))
            sched.step()
            
    
        # validation
        result = evaluate(model, val_loader)
        result['train_loss'] = torch.stack(train_losses).mean().item()
        result['lrs'] = lrs
        model.epoch_end(epoch, result)
        history.append(result)
        
    return history
    
epochs = 2
max_lr = 0.01
grad_clip = 0.1
weight_decay = 1e-4
opt_func = torch.optim.Adam

def plot_accuracies(history):
    accuracies = [x['val_accuracy'] for x in history]
    plt.plot(accuracies, '-x')
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.title('Accuracy vs. No. of epochs');

def plot_losses(history):
    def to_cpu(val):
        # Check if the value is a tensor, move to CPU if necessary, and get the item
        if isinstance(val, torch.Tensor):
            return val.cpu().item()
        return val  # If it's already a float, just return it
    
    train_losses = [to_cpu(x.get('train_loss')) for x in history]
    val_losses = [to_cpu(x['val_loss']) for x in history]
    
   
    
def plot_lrs(history):
    lrs = np.concatenate([x.get('lrs', []) for x in history])
    plt.plot(lrs)
    plt.xlabel('Batch no.')
    plt.ylabel('Learning rate')
    plt.title('Learning Rate vs. Batch no.');

plot_lrs(history)

test_dir = "../input/new-plant-diseases-dataset/test"
test = ImageFolder(test_dir, transform=transforms.ToTensor())

test_images = sorted(os.listdir(test_dir + '/test')) # since images in test folder are in alphabetical order
test_images

def predict_image(img, model):
    """Converts image to array and return the predicted class
        with highest probability"""
    # Convert to a batch of 1
    xb = to_device(img.unsqueeze(0), device)
    # Get predictions from model
    yb = model(xb)
    # Pick index with highest probability
    _, preds  = torch.max(yb, dim=1)
    # Retrieve the class label

    return train.classes[preds[0].item()]

# predicting first image
img, label = test[0]
plt.imshow(img.permute(1, 2, 0))
print('Label:', test_images[0], ', Predicted:', predict_image(img, model))


# getting all predictions (actual label vs predicted)
for i, (img, label) in enumerate(test):
    print('Label:', test_images[i], ', Predicted:', predict_image(img, model))

test_dir = "../input/sarojsaroj/test"
test = ImageFolder(test_dir, transform=transforms.ToTensor())


test_images = sorted(os.listdir(test_dir + '/test')) # since images in test folder are in alphabetical order
test_images


def predict_image(img, model):
    """Converts image to array and return the predicted class
        with highest probability"""
    # Convert to a batch of 1
    xb = to_device(img.unsqueeze(0), device)
    # Get predictions from model
    yb = model(xb)
    # Pick index with highest probability
    _, preds  = torch.max(yb, dim=1)
    # Retrieve the class label

    return train.classes[preds[0].item()]


# predicting first image
img, label = test[0]
plt.imshow(img.permute(1, 2, 0))
print('Label:', test_images[0], ', Predicted:', predict_image(img, model))

# saving to the kaggle working directory
PATH = './plant-disease-model.pth'  
torch.save(model.state_dict(), PATH)

# saving the entire model to working directory
PATH = './plant-disease-model-complete.pth'
torch.save(model, PATH)


import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os

# Define the path to the image
image_path = "../input/new-plant-diseases-dataset/test/test/AppleCedarRust1.JPG"

# Check if the file exists
if os.path.exists(image_path):
    # Load and display the image
    img = mpimg.imread(image_path)
    plt.imshow(img)
    plt.axis('off')  # Remove axis
    plt.show()
else:
    print("Image not found at the specified path.")

# getting all predictions (actual label vs predicted)
for i, (img, label) in enumerate(test):
    print('Label:', test_images[i], ', Predicted:', predict_image(img, model))

